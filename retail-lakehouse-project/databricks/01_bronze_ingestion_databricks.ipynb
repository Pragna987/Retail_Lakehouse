{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "762a4414",
   "metadata": {},
   "source": [
    "# Bronze Layer Ingestion - Databricks\n",
    "\n",
    "This notebook reads raw Instacart CSV files from DBFS and writes them to Delta Lake format in the Bronze layer.\n",
    "\n",
    "**Prerequisites:**\n",
    "- Cluster is running and attached\n",
    "- CSV files uploaded to `/FileStore/instacart/raw/`\n",
    "\n",
    "**Output:**\n",
    "- Delta tables in `/FileStore/instacart/bronze/`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "106ff67e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "RAW_PATH = \"/FileStore/instacart/raw\"\n",
    "BRONZE_PATH = \"/FileStore/instacart/bronze\"\n",
    "\n",
    "print(f\"Raw data path: {RAW_PATH}\")\n",
    "print(f\"Bronze output path: {BRONZE_PATH}\")\n",
    "print(f\"Spark version: {spark.version}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43691a15",
   "metadata": {},
   "source": [
    "## Verify Raw Files Exist\n",
    "\n",
    "Check if CSV files are uploaded to DBFS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d54bb0bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List files in raw directory\n",
    "raw_files = dbutils.fs.ls(RAW_PATH)\n",
    "\n",
    "print(\"Files in raw directory:\")\n",
    "for file in raw_files:\n",
    "    print(f\"  - {file.name} ({file.size / 1024 / 1024:.2f} MB)\")\n",
    "\n",
    "# Expected files\n",
    "expected_files = [\"orders.csv\", \"products.csv\", \"aisles.csv\", \n",
    "                  \"departments.csv\", \"order_products_train.csv\", \n",
    "                  \"order_products_prior.csv\"]\n",
    "\n",
    "missing_files = [f for f in expected_files if f not in [file.name for file in raw_files]]\n",
    "\n",
    "if missing_files:\n",
    "    print(f\"\\n‚ö†Ô∏è Missing files: {', '.join(missing_files)}\")\n",
    "    print(\"Upload missing files via Data ‚Üí Create Table ‚Üí Upload File\")\n",
    "else:\n",
    "    print(\"\\n‚úì All required files found!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9729d097",
   "metadata": {},
   "source": [
    "## Helper Function: Ingest CSV to Bronze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71b4f364",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import current_timestamp, lit\n",
    "\n",
    "def ingest_csv_to_bronze(csv_filename, table_name):\n",
    "    \"\"\"\n",
    "    Read CSV from DBFS and write to Bronze Delta table\n",
    "    \n",
    "    Args:\n",
    "        csv_filename: Name of CSV file (e.g., 'orders.csv')\n",
    "        table_name: Name for Bronze table (e.g., 'orders')\n",
    "    \"\"\"\n",
    "    raw_file_path = f\"{RAW_PATH}/{csv_filename}\"\n",
    "    bronze_table_path = f\"{BRONZE_PATH}/{table_name}\"\n",
    "    \n",
    "    print(f\"üì• Ingesting: {csv_filename} ‚Üí {table_name}\")\n",
    "    \n",
    "    try:\n",
    "        # Read CSV with header and schema inference\n",
    "        df = spark.read.csv(raw_file_path, header=True, inferSchema=True)\n",
    "        \n",
    "        # Add metadata columns\n",
    "        df_with_metadata = df \\\n",
    "            .withColumn(\"ingestion_timestamp\", current_timestamp()) \\\n",
    "            .withColumn(\"source_file\", lit(csv_filename))\n",
    "        \n",
    "        # Write to Delta Lake\n",
    "        df_with_metadata.write \\\n",
    "            .format(\"delta\") \\\n",
    "            .mode(\"overwrite\") \\\n",
    "            .save(bronze_table_path)\n",
    "        \n",
    "        record_count = df.count()\n",
    "        print(f\"   ‚úì Ingested {record_count:,} records\\n\")\n",
    "        \n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   ‚úó Error: {str(e)}\\n\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6fb1e1e",
   "metadata": {},
   "source": [
    "## Ingest All Tables\n",
    "\n",
    "Run ingestion for each CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c567c210",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"BRONZE LAYER INGESTION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "ingestion_tasks = [\n",
    "    (\"orders.csv\", \"orders\"),\n",
    "    (\"products.csv\", \"products\"),\n",
    "    (\"aisles.csv\", \"aisles\"),\n",
    "    (\"departments.csv\", \"departments\"),\n",
    "    (\"order_products_train.csv\", \"order_products_train\"),\n",
    "    (\"order_products_prior.csv\", \"order_products_prior\")\n",
    "]\n",
    "\n",
    "results = []\n",
    "for csv_file, table_name in ingestion_tasks:\n",
    "    success = ingest_csv_to_bronze(csv_file, table_name)\n",
    "    results.append((table_name, success))\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"INGESTION SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "successful = sum(1 for _, success in results if success)\n",
    "total = len(results)\n",
    "\n",
    "for table_name, success in results:\n",
    "    status = \"‚úì\" if success else \"‚úó\"\n",
    "    print(f\"{status} {table_name}\")\n",
    "\n",
    "print(f\"\\nCompleted {successful}/{total} ingestions successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05461730",
   "metadata": {},
   "source": [
    "## Verify Bronze Tables\n",
    "\n",
    "Check that Delta tables were created and preview data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a5218d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List Bronze tables\n",
    "bronze_tables = dbutils.fs.ls(BRONZE_PATH)\n",
    "\n",
    "print(\"Bronze Delta tables created:\")\n",
    "for table in bronze_tables:\n",
    "    print(f\"  - {table.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b50a468",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview orders table\n",
    "orders_df = spark.read.format(\"delta\").load(f\"{BRONZE_PATH}/orders\")\n",
    "\n",
    "print(f\"Orders table: {orders_df.count():,} records\")\n",
    "print(\"\\nSample data:\")\n",
    "display(orders_df.limit(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "207a0e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview products table\n",
    "products_df = spark.read.format(\"delta\").load(f\"{BRONZE_PATH}/products\")\n",
    "\n",
    "print(f\"Products table: {products_df.count():,} records\")\n",
    "print(\"\\nSample data:\")\n",
    "display(products_df.limit(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d47230a",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "‚úÖ **Bronze layer ingestion complete!**\n",
    "\n",
    "**Next steps:**\n",
    "1. Run `02_silver_transformation_databricks` to create cleaned, enriched tables\n",
    "2. Check Bronze tables in Data Explorer: `/FileStore/instacart/bronze/`"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
